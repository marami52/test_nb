{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regressions in R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<iframe width=\"800\" height=\"450\" src=\"https://www.youtube.com/embed/_QMC94WzcM0\" frameborder=\"0\" allowfullscreen></iframe>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LR as a basic of Machine Learning\n",
    "\n",
    "**What is machine learning?** \n",
    "It’s a Process for building a model\n",
    "\n",
    "**What is a model then?** \n",
    "It’s a function that associates  a response (y) with a set of observable parameters predictors (x1,x2,…): \n",
    "\n",
    "$$y= f\\left(\\vec{x}\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Properties of _f(x)_:\n",
    "\n",
    "- Practical! \n",
    "- easy to implement and explain\n",
    "- It should be single-valued - one output per set of x   vector\n",
    "- And of course, accurate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Such as Logistic Regressions!\n",
    "\n",
    "- One of the categorical models (binomial)\n",
    "\n",
    "$$logit(p)= ln\\left(\\frac{p}{1-p}\\right) = \\alpha + \\beta_1X_1 + ... + \\beta_kX_k  + \\epsilon$$\n",
    "\n",
    "- Why $logit$ function?\n",
    "${p}$ is 0 or 1 in our training dataset and is expected to be bounded to [0,1] in prediction where $logit(p)$ could be from $-\\infty$ to $\\infty$. \n",
    "\n",
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df <- mtcars[,c(\"am\", \"hp\",\"wt\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#when dependant variable (hp) is continuous\n",
    "library(ggplot2)\n",
    "ggplot(df,aes(wt,hp))+geom_point()+geom_smooth(method=\"lm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#when dependant variable (am) is binary and running OLS is not appropriate\n",
    "ggplot(df,aes(wt,am))+geom_point()+geom_smooth(method=\"lm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation in R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<iframe width=\"800\" height=\"450\" src=\"https://www.youtube.com/embed/AsLWQWwNvok\" frameborder=\"0\" allowfullscreen></iframe>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the prediction is lower than 0 and it doesn't look like a good fit at all.\n",
    "So, let's do it in the right way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit <- glm(data=df, am ~ wt , family= \"binomial\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(model.fit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$Null\\;Deviance = 2(LL(Saturated\\;Model)-LL(Null\\;Model))$<br>\n",
    "\n",
    "$Residual\\;Deviance = 2(LL(Saturated\\;Model)-LL(Fitted\\;Model))$<br>\n",
    "\n",
    "\n",
    "$$Pseudo\\;R^2= \\frac{Null\\;Deviance - Residual\\;Deviance}{Null\\;Deviance}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Sudo R2 is:\")\n",
    "(summary(model.fit)[[\"null.deviance\"]]-summary(model.fit)[[\"deviance\"]])/summary(model.fit)$null.deviance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head(predict(model.fit))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head(predict(model.fit, type=\"response\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance measurement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<iframe width=\"800\" height=\"450\" src=\"https://www.youtube.com/embed/SqipRlCK4pI\" frameborder=\"0\" allowfullscreen></iframe>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(caret)\n",
    "data(GermanCredit)\n",
    "df <- GermanCredit[,c(\"Class\",\"Age\",\"Amount\",\"Duration\")]\n",
    "df$Class <- ifelse(df$Class==\"Bad\" ,1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit <- glm(data = df, Class ~ Age + Amount + Duration , family = \"binomial\")\n",
    "summary(model.fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ClassHat <- predict(model.fit, type=\"response\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(pROC)\n",
    "g <- roc(Class ~ ClassHat , data = df)\n",
    "auc(g)\n",
    "plot(g, print.thres = \"best\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitted.results <- ifelse(ClassHat > 0.50,1,0)\n",
    "MyTable <- table(fitted.results, df$Class)\n",
    "caret::confusionMatrix(MyTable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat(\"Accuracy: \" , (35+673)/(27+265+35+673),\"\\n\")\n",
    "cat(\"Sensitivity: \" , 673/(27+673) ,\"\\n\")\n",
    "cat(\"Specifity\",35/(35+265),\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is Kappa?\n",
    "$$Kappa = \\frac{(observed\\;accuracy - expected\\;accuracy)}{(1 - expected\\;accuracy)}$$<br>\n",
    "\n",
    "In other word, it compares the performance of the model to a way of generating the results by chance. As a rule of thumb, Kappa > 0.75 assumed as an excellent performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multinomial Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<iframe width=\"800\" height=\"450\" src=\"https://www.youtube.com/embed/lNF62HbOOm8\" frameborder=\"0\" allowfullscreen></iframe>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multinomial Regression is an extension of logistic regression. It is used when the dependent variable is nominal with more than two levels.\n",
    "In fact, it's a series of logistic regressions in each one class is considered as the event (1) versus one particular class assumed as _pivot_. If we have $K$ classes in our case, we need to run $K-1$ logistic regressions as follows:\n",
    "\n",
    "$$ln\\left(\\frac{P(Y_i=1)}{P(Y_i=K)}\\right) = \\beta_{1}X_i$$<br>\n",
    "\n",
    "$$ln\\left(\\frac{P(Y_i=2)}{P(Y_i=K)}\\right) = \\beta_{2}X_i$$<br>\n",
    "\n",
    "$$...$$<br>\n",
    " \n",
    "$$ln\\left(\\frac{P(Y_i=K-1)}{P(Y_i=K)}\\right) = \\beta_{K-1}X_i$$<br>\n",
    "\n",
    "Considering that the sum of all probabilities is 1, we can derive that:\n",
    "\n",
    "$$P(Y_i=K-1) = \\frac{e^{\\beta_{K-1}X_i}}{\\sum_{j=1}^{K}{e^{\\beta_j X_i}}}$$\n",
    "\n",
    "It's also called **softmax** or **normalized exponential** function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use `mtcars` dataset in which we have cars with 3,4 and 5 gears. We would like to build a model to predict the gears based on the horse power (`hp`) and weight (`wt`). So, we have 3 classes in this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(nnet)\n",
    "df <- mtcars\n",
    "df$gear <- as.factor(df[[\"gear\"]])\n",
    "multi.nom <- multinom(gear ~ hp + wt, data= df, maxit=1000, reltol=1e-99, trace=FALSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(multi.nom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred <- predict(multi.nom, type=\"probs\")\n",
    "head(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we would like to generate the results as separate logistic regressions, First we need to generate different columns to have 3 separate classes: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gear.matrix <- model.matrix(~ gear  - 1, data=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfXtend <- cbind(df, gear.matrix)\n",
    "head(dfXtend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gear3.glm <- glm(gear3 ~ hp + wt, data=dfXtend, family=\"binomial\")\n",
    "summary(gear3.glm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gear4.glm <- glm(gear4 ~ hp + wt, data=dfXtend, family=\"binomial\")\n",
    "summary(gear4.glm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gear5.glm <- glm(gear5 ~ hp + wt, data=dfXtend, family=\"binomial\")\n",
    "summary(gear5.glm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define our softmax function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "softMax <- function(lr1, lr2, lr3){\n",
    "  \n",
    "  p1 <- exp(lr1) / ((exp(lr1) + exp(lr2) + exp(lr3)))\n",
    "  p2 <- exp(lr2) / ((exp(lr1) + exp(lr2) + exp(lr3)))\n",
    "  p3 <- exp(lr3) / ((exp(lr1) + exp(lr2) + exp(lr3)))\n",
    "  \n",
    "  data.frame(gear3=p1, gear4=p2, gear5=p3)\n",
    "  \n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm.pred <- softMax(predict(gear3.glm, type=\"link\"), predict(gear4.glm, type=\"link\"),  predict(gear5.glm, type=\"link\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head(sm.pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can test the ranking for all rows as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max.col(sm.pred)-max.col(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown, the ranking is similar for both models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ride"
  },
  "language_info": {
   "file_extension": ".r",
   "mimetype": "text/x-rsrc",
   "name": "r",
   "version": "R version 3.3.2 (2016-10-31)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
